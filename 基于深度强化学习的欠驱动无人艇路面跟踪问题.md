# 基于深度强化学习的欠驱动无人艇路面跟踪问题

## 摘要部分

论文的主要问题：基于深度强化学习的欠驱动无人艇路面跟踪问题

采用什么方法：基于深度Q网路（DQN）与强化学习的平滑收敛深度强化学习方法
	         **什么是DQN网络？深度学习与深度学习有什么区别嘛？**

新方法的改进：改进的DQN结构被开发为决策网络，以降低三自由度USV模型的路径跟踪控制律的复杂性。
	        提出了一种基于自适应梯度下降的探索函数，用于从经验数据中提取DQN的训练知识。
	       还设计了一个新的奖励函数来评估DQN的产出决策，从而加强控制USV路径的决策网络。
		**1、改进DQN结构网络，降低了USV模型路径跟踪运算的复杂度。**
		**2、提出一种新的自适应梯度下降的函数，用于训练其以往数据**
		**3、设计新的奖励函数来评估输出结果，加强USV路径的决策网络**

## 介绍

无人船应用广泛，但路径跟踪仍是一项具有挑战的任务。

**海洋运载器需要承受风、浪、涌的干扰，避免水中障碍物**

文献中已经做了大量工作来解决USV路径跟踪的控制问题[6]-[9]。

- Kapitanyuk等人使用引导矢量场方法实现了非完整移动机器人的路径跟踪控制。
- 福森和莱卡斯设计了一种基于USV相对速度的视线路径跟踪控制器。建立运动学模型，生成幅相形式的相对速度，利用水声测量将相对速度转化为绝对速度，实现控制规律。
- 凯姆什和贾那比-沙里菲建立了一个USV模型，基于线性二次调节器同时控制四轴飞行器和机械手。
- Hwang等人提出了一种基于模糊动态滑模控制的分层改进控制方法来跟踪自主地面车辆的路径。
- 蔡等人设计了基于事件触发模型的分布式预测控制框架，用于多无人机编队控制。
- 董等人利用基于状态反馈的反推控制解决了曲线和直线路径的跟踪问题。
- 郑和孙[12]提出了一种改进的自适应积分LOS制导律。控制框架基于反推技术，USV动态由径向基函数神经网络估计，控制器饱和由辅助系统解决。

在文献《Unmanned Aerial Vehicle Path Following：A Survey and Analysis of Algorithms for Fixed-Wing Unmanned Aerial Vehicless》中的一篇综述将USV路径跟踪的现有方法分为两组:

1. 几何分析：将虚拟目标点引入路径，并控制USV跟随虚拟点以确保正确的路径跟随。
2. 另基于控制的分析。采用各种控制方法，如线性二次调节器、滑模控制、模型预测控制、反推控制、增益调度理论、自适应控制、动态规划和变结构控制来实现路径跟踪。

然而，这两个组本质上都是建立在数学估计分析的基础上的，这可能会受到一些固有的限制，例如高计算复杂性、较差的可移植性和环境干扰。先前的研究证明了基于数学估计的方法在USV路径跟踪中有效性能，但在实际操作中很难建立相应的数学模型。

随着Google DeepMind项目在深度密集学习中的突破，强化学习得到了广泛的应用。强化学习被认为是设计智能系统的核心技术之一。结合深度学习，深度强化学习（DRL）能够在动态环境中表征和控制极其复杂的系统。因此，为有效的路径跟随控制开发基于DRL的决策模型是合理的。

- 徐等提出了一种基于DRL和奖励函数的路径规划算法。
- 程和张基于DRL研究了未知环境干扰下欠驱动USV的避障问题。使用深层Q网络开发了简洁的DRL，以克服传统分析方法中复杂的控制定律引起的可用性问题。 
- Li等使用RL减少了高速公路瓶颈处的旅行时间。提出了一种基于Q学习的RL模型来控制各种交通状态下车辆的速度限制。使用基于Q学习的RL可以显着减少行程时间。

然而，Q训练的训练效率是一个问题，而文献中很少讨论基于Q学习的USV路径跟踪RL的改进。值得研究用于USV路径跟踪的有效DRL模型。

## USV 路径规划

**路径规划**：持续控制无人船按照预先设定的路线行走。

**路径规划的关键**：在路径跟踪的进程中，环境中不确定和分散的因素会使无人艇运动的状态改变，所以，必须消除动态的误差干扰，防止USV偏离预设路径。

**USAV的3-DoF的动态模型。地面惯性框架和车身框架中的USV动力学可以描述为：
$$
\overset{·}{\eta}\,\,=\,\,R\left( \psi \right) v
$$
其中，$$
\eta \,\,=\left[ x,y,\psi \right] \,\,
$$ 和  $$v\,\,=\left[ u,v,r \right] ^T\,\,$$ 分别表示位置矢量与速度矢量。项$$\left( x,y \right) $$ 是USV在固定惯性框架中的位置， $$
\psi $$ 是航向角；u, v和r是USV在固定车架中的浪涌，摇摆和航向运动。3自由度旋转矩阵R($$ \psi$$)将船体固定框架中的矢量转换为固定在地球上的惯性框架：
$$
R\left( \psi \right) =\left[ \begin{array}{c}
	\begin{matrix}
	\begin{matrix}
	\cos \psi&		-\sin \psi\\
\end{matrix}&		0\\
\end{matrix}\\
	\begin{matrix}
	\begin{matrix}
	\sin \psi&		\cos \psi\\
\end{matrix}&		0\\
\end{matrix}\\
	\begin{matrix}
	\begin{matrix}
	0&		0\\
\end{matrix}&		1\\
\end{matrix}\\
\end{array} \right]
$$
USV的动态运动方程可以表示为：
$$
M\overset{·}{v}+C\left( v \right) +Dv\,\,=\,\,\tau +\tau _w
$$
其中，$$M\,\,=\,\,M_A+M_{RB}$$ 其中$$M_A$$和$$M_{RB}$$ 分别是附加质量矩阵与刚性质量矩阵。C(v)表示Coriolis和centripetal term。D是非线性阻尼矩阵；$$\tau \,\,=\,\,\left[ \tau _u\,\,, 0 , \tau _r \right]^T $$ 和 $$\tau \,\,=\,\, \left[ \tau _{wu}\,\,, \tau _{wv}\,\,, \tau _{wr} \right] ^T$$ 分别表示控制输入和未知变量扰动。$$\tau _u$$ 表示surge force  和$$\tau _r$$ 表示偏航角。$$\tau _{wu}\,\,, \tau _{wv}\,\,, \tau _{wr}$$ 分别表示滚动，俯仰和偏航的干扰。



### B 控制目标

综合了干扰的随机性，以充分表征环境变化

在这项研究中，使用了机器学习技术来设计决策系统。如图2所示，USV的运动状态和路径跟随状态被集成为决策网络的输入。网络的输出是控制命令。需要修改控制目标，以使其易于训练和调整决策网络。假设控制错误为

路径跟踪的目的是希望USV与设计路径之间的跨越误差控制在一定的小范围内。并定义假设了滚动，俯仰和偏航扰动的输入函数来表现USV在航向中受到的不确定因素。其中跟随误差e的路径为 $$e\,\,=\,\,w_de_d\,\,+\,\,w_{\psi}e_{\psi}$$ 。如果USV接近设计路径 则$$e_d$$和$$e_{\psi}$$ 具有不同的符号，e值接近0. 如果USV偏离了设计的路径，那么$$e_d$$和$$e_{\psi}$$ 具有相同的符号。随后加权$$e_d$$和$$e_{\psi}$$ 的总和，以综合的减少表示接近路径，综合的增加表示偏离路径。

## 使用SCDRL(Smoothly—Convergent Deep Reinforcement Learning)方法进行路径跟踪

### **Q-Learning**

#### 行为准则

​	我们做事情都会有一个自己的行为准则, 比如小时候爸妈常说”不写完作业就不准看电视”. 所以我们在 写作业的这种状态下, 好的行为就是继续写作业, 直到写完它, 我们还可以得到奖励, 不好的行为 就是没写完就跑去看电视了, 被爸妈发现, 后果很严重. 小时候这种事情做多了, 也就变成我们不可磨灭的记忆. 这和我们要提到的 Q learning 有什么关系呢? 原来 Q learning 也是一个决策过程, 和小时候的这种情况差不多. 我们举例说明.

假设现在我们处于写作业的状态而且我们以前并没有尝试过写作业时看电视, 所以现在我们有两种选择 , 1, 继续写作业, 2, 跑去看电视. 因为以前没有被罚过, 所以我选看电视, 然后现在的状态变成了看电视, 我又选了 继续看电视, 接着我还是看电视, 最后爸妈回家, 发现我没写完作业就去看电视了, 狠狠地惩罚了我一次, 我也深刻地记下了这一次经历, 并在我的脑海中将 “没写完作业就看电视” 这种行为更改为负面行为, 我们在看看 Q learning 根据很多这样的经历是如何来决策的吧.

#### Q-learning决策

假设我们的行为准则已经学习好了, 现在我们处于状态s1, 我在写作业, 我有两个行为 a1, a2, 分别是看电视和写作业, 根据我的经验, 在这种 s1 状态下, a2 写作业 带来的潜在奖励要比 a1 看电视高, 这里的潜在奖励我们可以用一个有关于 s 和 a 的 Q 表格代替, 在我的记忆Q表格中, Q(s1, a1)=-2 要小于 Q(s1, a2)=1, 所以我们判断要选择 a2 作为下一个行为. 现在我们的状态更新成 s2 , 我们还是有两个同样的选择, 重复上面的过程, 在行为准则Q 表中寻找 Q(s2, a1) Q(s2, a2) 的值, 并比较他们的大小, 选取较大的一个. 接着根据 a2 我们到达 s3 并在此重复上面的决策过程. Q learning 的方法也就是这样决策的. 看完决策, 我看在来研究一下这张行为准则 Q 表是通过什么样的方式更改, 提升的.

### **DQN中神经网络的作用**

​			我们使用表格来存储每一个状态 state, 和在这个 state 每个行为 action 所拥有的 Q 值. 而当今问题是在太复杂, 状态可以多到比天上的星星还多(比如下围棋). 如果全用表格来存储它们, 恐怕我们的计算机有再大的内存都不够, 而且每次在这么大的表格中搜索对应的状态也是一件很耗时的事. 不过, 在机器学习中, 有一种方法对这种事情很在行, 那就是神经网络. 我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 这样我们就没必要在表格中记录 Q 值, 而是直接使用神经网络生成 Q 值. 还有一种形式的是这样, 我们也能只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作. 我们可以想象, 神经网络接受外部的信息, 相当于眼睛鼻子耳朵收集信息, 然后通过大脑加工输出每种动作的值, 最后通过强化学习的方式选择动作.



DQN的好处

DL与RL结合存在以下问题 ：

- DL是监督学习需要学习训练集，强化学习不需要训练集只通过环境进行返回奖励值reward，同时也存在着噪声和延迟的问题，所以存在很多状态state的reward值都是0也就是样本稀疏
- DL每个样本之间互相独立，而RL当前状态的状态值是依赖后面的状态返回值的。
- 当我们使用非线性网络来表示值函数的时候可能出现不稳定的问题

DQN中的两大利器解决了以上问题

- 通过Q-Learning使用reward来构造标签
- 通过experience replay（经验池）的方法来解决相关性及非静态分布问题
- 使用一个MainNet产生当前Q值，使用另外一个Target产生Target Q



​		DRL是深度学习和RL的结合。通常，RL利用知识并将学习结果存储在Q表中[41]。由于Q表的空间限制，RL的学习能力受到限制，导致在处理具有大量空间状态的复杂系统时效率低下和无效。幸运的是，DRL能够通过执行深度学习来解决高维问题。相对于RL，DRL的优势如下所述[42]。首先，RL使用Q表存储学习的经验，而DRL使用神经网络来近似学习的经验。其次，DRL可以从高维输入中快速学习控制知识和规则，而RL很难从高维数据中学习知识。结果，在这项研究中，将深度Q网络（DQN）和RL集成在一起，以建立USV路径跟踪的DRL模型。（**什么是Q-Learning？什么是强化学习？**） 



### A 路径跟踪结构概述

<img src="G:\基础课件\海洋运载器\基于深度强化学习的欠驱动无人听路径路线跟踪问题\Fig3 .JPG" style="zoom:67%;" /> 



上图展示了所提出的路径跟踪方法的概述。

- 首先，USV的观测状态包括运动状态和环境状态被用来训练决策网络。
- 然后，决策网络选择正确的控制动作来调整螺旋桨和转向器以遵循设计路径。 DRL方法用于训练决策网络。在训练过程开始时，为决策网络随机选择初始参数。从图3的右侧可以看出，当决策网络做出决策时，训练程序必须获得受决策影响的状态，并根据奖励函数计算即时奖励和长期奖励。
- 然后，将评估后的决策奖励通过损失函数反馈到决策网络，以提高网络的决策能力。在这项研究中，DQN被用作决策网络，以学习遵循知识的道路并选择正确的行动来应对USV的不同状态。

在操作过程中，DQN可以根据不同的USV运动和环境状态选择一系列相应的动作。然后，USV将通过执行操作来完成遵循的路径。运动和环境状态信息由各种传感器（例如全球定位系统，地磁模块和速度计）测量。为了避免测量噪声，在输入DQN模型之前将这些感官测量标准化。

**相关函数的定义和含义**

1. 定义$$
   S\,\,=\,\,\left\{ s\left( u,v,r,x,y,\psi ,\rho ,e_d,e_{\psi},v_d \right) \right\} 
   $$ 为状态空间；
2. 定义$$
   A\,\,=\,\,\left\{ a \right\} 
   $$ 为执行动作空间；
3. 定义$$
   R\,\,=\,\,\left\{ r \right\} 
   $$ 为奖励集；
4. 定义$$A\,\,\left(s\right)$$ 表示在状态s下的所有动作；
5. $$S_t,A_t,R_t和G_t$$表示状态，动作的选择，奖励和步长t步内的奖励累积；
6.  $$\pi$$指的是在状态s下选择的策略；
7. $$\pi \left( s \right) $$ 是指在状态s下根据策略$$\pi$$选择的动作；
8. $$\pi_*$$指的是最优的策略；
9. $$\pi \left( a\,\,| s \right) 
   $$ 指的是在状态s根据策略$$\pi$$执行动作a的概率；
10. $$r(s,a)$$在状态s情况下选择动作a的奖励
11. $$r(s,a,s^`)$$ 是在状态s下选择动作a的奖励，然后将状态变为s` 
12. $$v_{\pi}(s)$$是状态价值函数，表示在状态s根据策略$$\pi$$下累计获得的奖励$$G_t$$
13. $$q_{\pi}(s,a)$$是动作价值函数，表示在状态s下选择动作a所累计获得的奖励$$G_t$$
14. $$v_*(s)和q_*(s,a)$$分别表示最有状态价值和最优动作值
15. $$V(s)=\left\{v_{\pi}(s)\right\}$$表示最优状态价值集
16. $$Q(s，a)=\left\{q_{\pi}(s，a)\right\}$$表示最优动作价值集
17. $$p(s^`,r\,|\,s,a)$$表示在状态s下选择动作a并将状态转变为$$s^`$$所获得奖励r的概率

**————————————数学公式具体定义推导跳过——————————————**

### B  基于平滑收敛的深度强化学习的决策系统

<img src="G:\基础课件\海洋运载器\基于深度强化学习的欠驱动无人听路径路线跟踪问题\Fig. 4. JPG.JPG" style="zoom:80%;" />

<img src="G:\基础课件\海洋运载器\基于深度强化学习的欠驱动无人听路径路线跟踪问题\Fig. 5. JPG.JPG" style="zoom:80%;" />

​		图4描述了所提出的基于SCDRL的USV路径跟踪方法的框图。

​		本研究引入双DQN学习结构作为决策系统。如图4所示，两个DQN模型平行排列。在线DQN被用作决策者，而目标DQN被用作镜子。SCDRL的输入是当前的观测状态，下一步的观测状态代表环境扰动。在输入和DQN决策者之间，设计了一个探索功能，以丰富DQN学习的国家知识。在线DQN的输出动作指示USV的推力和方向舵角度。执行器将驱动转向机和电机执行动作。目标DQN首先保存在线DQN的当前网络参数，然后使用体验重放池来更新参数。最后，目标DQN将把更新后的参数传送到在线DQN。目标DQN将在预设间隔内更新模型参数奖励函数用于执行强化学习。输出动作的奖励被输入到经验重放池，以学习新知识和更新决策系统的参数。

​			决策DQN的结构如图5所示。输入包括运动状态u、v、r、x、y、路径参数ρ、交叉轨迹误差ed、航向误差eψ和设计速度νd。通过两个卷积层和三个完全连接的层，最终可以输出特定状态下每个动作的动作值。决策是基于行动价值做出的。

​		  建议的SCDRL路径跟踪控制器可以安装在船上，而培训过程不必在USV进行。例如，SCDRL控制器的训练可以在岸基控制站中实现，然后，获得的控制器参数可以被传输到机载系统。由于高端电脑和业余训练，岸基控制站训练过程中的计算负担非常有限。

### C 奖励函数

​		奖励功能在RL中起着重要作用[43]，而在强化系统中，设计奖励机制总是很困难[44]。一个合适的奖励函数设计将能够评估DQN当前的决策，并向系统反馈新的知识，以提高DQN的自学能力。由于奖励机制，DQN能够做出最优决策来控制USV路径的遵循。

​		在USV路径跟踪任务中，期望奖励函数满足三个要求：

- (1)激励USV沿着设计的路径航行；
- (2)激励USV以设计速度行驶；
- (3)激励USV克服环境干扰。

1）为了推动USV前进，将导航奖励 $$R_N$$定义为路径参数ρ的函数。如果USV沿设计路径行驶，则增加

2）为了减少USV和设计路径之间的跨轨误差，设计了误差奖励 $$R_E$$。 $$R_E$$是USV的位置（x，y）与设计路径P（ρt）之间的距离的函数，其中ρ是时间t的ρ值。

​		航向误差可以忽略。为了解决这个问题，对距离误差和航向误差进行加权，以提高跟踪设计路径的报酬。通过这样做，增强了路径跟踪算法的抗干扰能力，并减小了跨轨误差。更新后的$$R_E$$可以表示为



## 算法描述：

<img src="G:\基础课件\海洋运载器\基于深度强化学习的欠驱动无人听路径路线跟踪问题\Fig. 6. JPG.JPG" style="zoom:150%;" />

**修改1**   $$
\nabla _{\epsilon -\min}
$$ 和 k是常数； ∇-使每次训练保证一个最小下降梯度； k是学习率； $$e_m$$是最大跨轨误差。一个训练单元中每一步的平均奖励被用作评估学习质量的标准，因此，平均奖励的自然对数被用于将评估标准限制在[0，em]的范围内。根据评估标准，在训练过程中，下降梯度将缓慢响应较小的标准值，而快速响应较大的标准值。结果，与恒定下降梯度率相比，该自适应策略可以有效地减少计算量。

**修改2** π（φi|）和y的共域对成本非常重要。由于计算机采用二进制运算，因此，如果共域值太大或太小，则计算量将增加。为了解决这个问题，奖励函数在方程式中。 （30）-式设计了（34），其中rNis用于使USV向前移动，rVis用于保持正常速度，而rE中使用对数和指数形式来保持rEwithin在[0,1]之内。这样一来，可以减少不良的路径跟随体验的计算成本，并且可以学习有用的路径跟随体验。结果，期望培训过程是有效的。



### **算法评估：**

​		如果没有足够的训练来控制USV，决策网络将无法做出正确的决策。经过短暂的培训，决策网络学会了一定的路径跟随能力，但还不够。如图6（b）和图7（b）所示，USV仍然无法完成路径跟随任务。但是，经过500次训练后还节省时间。从图6（d）和图7（d）可以看出，最终的训练结果是令人满意的。决策网络可以做出正确的决策。 USV可以遵循完整的路径，并具有可接受的微小错误。

​		图8-11显示了以下示例中两条路径中的轨迹误差和航向角的变化。在图8和9中，x轴是采样数，y轴是误差距离。在训练过程中，DQN逐渐学会了缩小误差范围，并通过充分的训练消除了跨轨误差。可以在图1和2中看到。从图10和11可以看出，USV航向角的变化随着图11逐渐变得平滑。训练时间的增加，变化范围逐渐减小，可以保证导航的安全性。控制结果见图8。图11证明了所提出的决策网络能够在没有任何先验知识和经验的情况下遵循规则学习路径。

​		此外，为了突出显示SCDRL的一般性，测试了由几个段和曲线组成的两个复杂路径。第一个结合了多边形曲线和正弦曲线，图16显示了路径的轨迹。第二个采用[32]中介绍的路径，图17描绘了路径跟随结果。从图5和6中可以看出。如图16和17所示，所提出的SCDRL方法可以很好地遵循设计的复杂路径。无花果图18和19分别显示了在两种复杂路径下的速度和跨轨误差。可以注意到，USV可以很好地遵循设计的路线。即使在严重改变路径方向时也是如此。提出的SCDRL方法具有出色的自适应能力，可以遵循不同的路径。

### **总结**

开发了一种二维DQN结构作为决策系统，以学习和更新路径遵循的控制法则的决策规则。设计了改进的探索功能和奖励功能，以使DQN模型的训练和增强顺利收敛。数值仿真验证了该方法的有效性。分析结果表明，所提出的SCDRL能够跟踪各种预先设计的路径，且道间交叉误差小于传统的DRL方法。未来的工作将把SCDRL方法应用于实验性USV模型以及实际的USV。